# Steps for differents parts of the pipeline

### Preprocessing data
1. Scripts can be found in `preprocessing` directory
1. The cached data gets stored in - `/home/asd/.cache/huggingface/datasets/text/`. So make sure there is enough space on the disk

### Create vocabulary files
1. `python create_tokenizer.py --file ../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.train.raw --store_files ../config/roberta_test/ --vocab_size 50000`

### Galactic dependencies
1. Example data for galactic dependencies is in `/n/fs/nlp-asd/asd/asd/BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/galactic/treebanks-V1.0/treebanks`
1. Converting a CONLLU file to a different word order - `GALACTIC_ROOT=$(pwd) bin/gd-translate --input ../../../../data/dependency_parse_data/english/dep_wiki.valid.raw --spec en~fr@N~hi@V`. Run from `preprocessing/dependency_parsing/gdtreebank`

### Running code on TPUs
1. MLM RoBERTa-base - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --output_dir=../../../bucket/model_outputs/wikitext/mono_english_40 --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 20 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name wikitext_mlm   &`
1. MLM with own vocab - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --output_dir=../../../bucket/model_outputs/wikitext/mono_english --model_type=roberta --config_name=roberta-base --tokenizer_name=../config/roberta_test --learning_rate 1e-4 --num_train_epochs 20 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name wikitext_mlm   &`
1. Monolingual French model - `nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --output_dir=../../../bucket/model_outputs/french/mono_french_warmup --model_type=roberta --config_name=camembert-base --tokenizer_name=camembert-base --num_train_epochs 80 --warmup_steps 10000 --learning_rate 1e-4 --do_train --do_eval  --train_file=../../../bucket/french_data/processed/fr_gutenberg/train.txt --validation_file=../../../bucket/french_data/processed/fr_gutenberg/valid.txt --cache_dir=../../../bucket/cache --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name monolingual_french &`

### Word based modifications
1. Vocabulary permutation TPU - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --data_cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_vocab_permutation --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 20000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name word_permute_40 --permute_vocabulary --vocab_permutation_file ../synthetic_language_files/word_based/configuration_files/permuted_vocab_seed_42_size_50265.json --vocab_modification random    &`
1. Vocabulary permutation TPU without `data_cache_dir` - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_vocab_permutation --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 60 --warmup_steps 20000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name word_permute_60 --permute_vocabulary --vocab_permutation_file ../synthetic_language_files/word_based/configuration_files/permuted_vocab_seed_42_size_50265.json --vocab_modification add    &`
1. Resume from checkpoint - `nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --model_name_or_path=../../../bucket/model_outputs/wikitext/english_vocab_permutation/checkpoint-20000/ --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_vocab_permutation --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 20000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name permute_vocabulary --permute_vocabulary --vocab_permutation_file ../synthetic_language_files/word_based/configuration_files/permuted_vocab_seed_42_size_50265.json --vocab_modification add &`
1. Word modification TPU without `data_cache_dir` - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_word_modification_095 --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 40 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name word_modification_40 --modify_words --modify_words_probability 0.95 --word_modification add   &`
1. Inverted word order - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_word_modification_inverted --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 40 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name inverted_sentence_40  --invert_word_order --word_modification add   &`
1. Inverted order French - `nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --output_dir=../../../bucket/model_outputs/french/french_inverted_warmup --model_type=roberta --config_name=camembert-base --tokenizer_name=camembert-base --num_train_epochs 40 --warmup_steps 10000 --learning_rate 1e-4  --do_train --do_eval  --train_file=../../../bucket/french_data/processed/fr_gutenberg/train.txt --validation_file=../../../bucket/french_data/processed/fr_gutenberg/valid.txt --cache_dir=../../../bucket/cache --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name inverted_french --invert_word_order --word_modification add &`
1. One-to-one mapping - `nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_word_modification_one_to_one --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 40 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name one_to_one_mapping_40 --one_to_one_mapping --word_modification add  &`

### Dependency parse of the corpus
1. Command for converting a corpus to CONLLU file (MLM) - `python convert_dataset_to_dependency.py --language en --data /n/fs/nlp-asd/asd/asd/BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.raw --save_dir /n/fs/nlp-asd/asd/asd/Projects/Multilingual/data/dependency_parse_data/english`

### Syntax modifications
1. Monolingual model on English Conllu - `nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm.py --output_dir=../../../bucket/model_outputs/wikitext/mono_english_syntax --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --num_train_epochs 2400 --do_train  --train_file=../../../bucket/syntax_modified_data/english_english.txt --run_name mono_english_syntax --save_steps 10000 --logging_steps 5 &`
1. English + Hindi - `nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm.py --output_dir=../../../bucket/model_outputs/wikitext/english_syntax_hindi --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --num_train_epochs 2400 --do_train  --train_file=../../../bucket/syntax_modified_data/english_modified.txt --run_name english_syntax_hindi --save_steps 10000 --logging_steps 5 &`