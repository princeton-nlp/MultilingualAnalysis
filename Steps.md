# Steps for differents parts of the pipeline

### Preprocessing data
1. Scripts can be found in `preprocessing` directory
1. The cached data gets stored in - `/home/asd/.cache/huggingface/datasets/text/`. So make sure there is enough space on the disk

### Create vocabulary files
1. `python create_tokenizer.py --file ../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.train.raw --store_files ../config/roberta_test/ --vocab_size 50000`

### Running code on TPUs
1. MLM Monolingual - `python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm.py --output_dir=../../../bucket/model_outputs/wikitext/mono_english --model_type=roberta --config_name=../config/roberta_8_orig/ --tokenizer_name=../config/roberta_8_orig/ --num_train_epochs 20 --do_train --do_eval  --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --save_steps 10000`
1. MLM RoBERTa-base - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --output_dir=../../../bucket/model_outputs/wikitext/mono_english_40 --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 20 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name wikitext_mlm   &`
1. MLM with own vocab - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --output_dir=../../../bucket/model_outputs/wikitext/mono_english --model_type=roberta --config_name=roberta-base --tokenizer_name=../config/roberta_test --learning_rate 1e-4 --num_train_epochs 20 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name wikitext_mlm   &`

### Word based modifications
1. Vocabulary permutation TPU - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --data_cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_vocab_permutation --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 20000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name word_permute_40 --permute_vocabulary --vocab_permutation_file ../synthetic_language_files/word_based/configuration_files/permuted_vocab_seed_42_size_50265.json --vocab_modification random    &`
1. Vocabulary permutation TPU without `data_cache_dir` - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_vocab_permutation --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 60 --warmup_steps 20000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name word_permute_60 --permute_vocabulary --vocab_permutation_file ../synthetic_language_files/word_based/configuration_files/permuted_vocab_seed_42_size_50265.json --vocab_modification add    &`
1. Resume from checkpoint - `nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --model_name_or_path=../../../bucket/model_outputs/wikitext/english_vocab_permutation/checkpoint-20000/ --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_vocab_permutation --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 20000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name permute_vocabulary --permute_vocabulary --vocab_permutation_file ../synthetic_language_files/word_based/configuration_files/permuted_vocab_seed_42_size_50265.json --vocab_modification add &`
1. Word modification TPU without `data_cache_dir` - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_word_modification_095 --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 40 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name word_modification_40 --modify_words --modify_words_probability 0.95 --word_modification add   &`
1. Inverted word order - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_word_modification_inverted --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 40 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name inverted_sentence_40  --invert_word_order --word_modification add   &`
