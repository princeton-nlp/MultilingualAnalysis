# Steps for differents parts of the pipeline

### Preprocessing data
1. Scripts can be found in `preprocessing` directory
1. The cached data gets stored in - `/home/asd/.cache/huggingface/datasets/text/`. So make sure there is enough space on the disk

### Create vocabulary files
1. `python create_tokenizer.py --file ../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.train.raw --store_files ../config/roberta_test/ --vocab_size 50000`

### Debugging on cycles.princeton.edu
1. MLM Monolingual - `python examples/language-modeling/run_mlm.py --train_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --output_dir=../../data/model_outputs/wikitext/debug --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 2 --warmup_steps 10000 --do_train --save_steps 10000 --per_device_train_batch_size 2 --overwrite_output_dir`
1. MLM vocabulary permute - `python -m pdb examples/language-modeling/run_mlm_synthetic.py --train_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --output_dir=../../data/model_outputs/wikitext/debug --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 2 --warmup_steps 10000 --do_train --save_steps 10000 --per_device_train_batch_size 2 --overwrite_output_dir --permute_vocabulary --vocab_permutation_file /n/fs/nlp-asd/asd/asd/Projects/Multilingual/Multilingual/synthetic_language_files/word_based/configuration_files/permuted_vocab_seed_42_size_50265.json --vocab_modification random` and breakpoint at `b ../../src/transformers/synthetic_language_modifications_utils.py:25`

### Running code on TPUs
1. MLM Monolingual - `python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm.py --output_dir=../../../bucket/model_outputs/wikitext/mono_english --model_type=roberta --config_name=../config/roberta_8_orig/ --tokenizer_name=../config/roberta_8_orig/ --num_train_epochs 20 --do_train --do_eval  --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --save_steps 10000`
1. MLM RoBERTa-base - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --output_dir=../../../bucket/model_outputs/wikitext/mono_english_40 --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 20 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name wikitext_mlm   &`
1. MLM with own vocab - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --output_dir=../../../bucket/model_outputs/wikitext/mono_english --model_type=roberta --config_name=roberta-base --tokenizer_name=../config/roberta_test --learning_rate 1e-4 --num_train_epochs 20 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name wikitext_mlm   &`

### Word based modifications
1. Vocabulary permutation - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --output_dir=../../../bucket/model_outputs/wikitext/english_vocab_permutation --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 20000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name mutlilingual_word --permute_vocabulary --vocab_permutation_file ../synthetic_language_files/word_based/configuration_files/permuted_vocab_seed_42_size_50265.json --vocab_modification random    &`
