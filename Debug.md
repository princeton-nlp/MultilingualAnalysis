# Shell commands for debugging

### Debugging on cycles.princeton.edu
1. MLM Monolingual - `python examples/language-modeling/run_mlm.py --train_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --output_dir=../../data/model_outputs/wikitext/debug --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 2 --warmup_steps 10000 --do_train --save_steps 10000 --per_device_train_batch_size 2 --overwrite_output_dir`
1. MLM vocabulary permute - `python -m pdb examples/language-modeling/run_mlm_synthetic.py --train_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --validation_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --output_dir=../../data/model_outputs/wikitext/debug --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 2 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --per_device_train_batch_size 2 --overwrite_output_dir --permute_vocabulary --vocab_permutation_file /n/fs/nlp-asd/asd/asd/Projects/Multilingual/Multilingual/synthetic_language_files/word_based/configuration_files/permuted_vocab_seed_42_size_50265.json --word_modification add`
1. Random word modification - `python -m pdb examples/language-modeling/run_mlm_synthetic.py --train_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --validation_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --output_dir=../../data/model_outputs/wikitext/debug --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 2 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --per_device_train_batch_size 2 --overwrite_output_dir --modify_words --word_modification add`
1. Inverting sentence - `python -m pdb examples/language-modeling/run_mlm_synthetic.py --train_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --validation_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --output_dir=../../data/model_outputs/wikitext/debug --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 2 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --per_device_train_batch_size 2 --overwrite_output_dir --invert_word_order --word_modification add`
1. Inverting sentence with cache - `python -m pdb examples/language-modeling/run_mlm_synthetic.py --train_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --validation_file=../../../../BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/wikitext-103-raw/wiki.valid.txt --cache_dir /n/fs/nlp-asd/asd/asd/BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/transformer_models --output_dir=../../data/model_outputs/wikitext/debug --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 2 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --per_device_train_batch_size 2 --overwrite_output_dir --invert_word_order --word_modification add`