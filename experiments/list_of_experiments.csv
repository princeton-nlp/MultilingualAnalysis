User,Date,VM,TPU,Description,Command,W&B link
Ameet,3/20/2021,mult-p-1,tpu-1,[one_to_one_mapping_80] Resuming one_to_one_mapping_80 run from checkpoint-170000,nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --model_name_or_path=../../../bucket/model_outputs/wikitext/eng_word_modif_one_to_one_80/checkpoint-170000 --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/eng_word_modif_one_to_one_80_re --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name one_to_one_mapping_80 --one_to_one_mapping --word_modification add  &, https://wandb.ai/ameet-1997/mutlilingual_word/runs/tqbz2b8l?workspace=user-ameet-1997
Ameet,3/20/2021,mult-p-2,tpu-2,[syntax_english_mono_240] English Mono model with 240 epochs on data which Galactic code has parsed,nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/syntax_modified_data/english/mono_dep_en_train.txt --validation_file=../../../bucket/syntax_modified_data/english/mono_dep_en_train.txt --output_dir=../../../bucket/model_outputs/syntax_modif/english/mono_english_240 --model_type=roberta --config_name=../config/roberta_8/roberta_base_8_512.json --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 240 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name syntax_english_mono_240   &, https://wandb.ai/ameet-1997/mutlilingual_syntax/runs/q19f430y?workspace=user-ameet-1997
Ameet,3/21/2021,mult-p-2,tpu-2,[syntax_english_en_frN_hiV_120] English synthetic model with 120 epochs on synthetic language,nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/syntax_modified_data/english/synthetic_en_frN_hiV.txt --validation_file=../../../bucket/syntax_modified_data/english/mono_dep_en_valid.txt --output_dir=../../../bucket/model_outputs/syntax_modif/english/english_synth_en_fr_hi_120 --model_type=roberta --config_name=../config/roberta_8/roberta_base_8_512.json --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 120 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name syntax_english_en_frN_hiV_120   &, Running