# Commands used for all the experiments

1. RoBERTa 8 layer version on English-dep data (smaller dataset) monolingual model - `nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/syntax_modified_data/english/mono_dep_en_train.txt --validation_file=../../../bucket/syntax_modified_data/english/mono_dep_en_train.txt --output_dir=../../../bucket/model_outputs/syntax_modif/english/mono_english_80 --model_type=roberta --config_name=../config/roberta_8/roberta_base_8_512.json --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 10000 --do_train --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name syntax_english_mono_80   &`