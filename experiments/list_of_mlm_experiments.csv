User,Date,VM,TPU,Description,Command,W&B link

Ameet,3/20/2021,mult-p-1,tpu-1,[one_to_one_mapping_80] Resuming one_to_one_mapping_80 run from checkpoint-170000,nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --model_name_or_path=../../../bucket/model_outputs/wikitext/eng_word_modif_one_to_one_80/checkpoint-170000 --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/eng_word_modif_one_to_one_80_re --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name one_to_one_mapping_80 --one_to_one_mapping --word_modification add  &, https://wandb.ai/ameet-1997/mutlilingual_word/runs/tqbz2b8l?workspace=user-ameet-1997

Ameet,3/20/2021,mult-p-2,tpu-2,[syntax_english_mono_240] English Mono model with 240 epochs on data which Galactic code has parsed,nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/syntax_modified_data/english/mono_dep_en_train.txt --validation_file=../../../bucket/syntax_modified_data/english/mono_dep_en_train.txt --output_dir=../../../bucket/model_outputs/syntax_modif/english/mono_english_240 --model_type=roberta --config_name=../config/roberta_8/roberta_base_8_512.json --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 240 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name syntax_english_mono_240   &, https://wandb.ai/ameet-1997/mutlilingual_syntax/runs/q19f430y?workspace=user-ameet-1997

Ameet,3/21/2021,mult-p-2,tpu-2,[syntax_english_en_frN_hiV_120] English synthetic model with 120 epochs on synthetic language,nohup   python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/syntax_modified_data/english/synthetic_en_frN_hiV.txt --validation_file=../../../bucket/syntax_modified_data/english/mono_dep_en_valid.txt --output_dir=../../../bucket/model_outputs/syntax_modif/english/english_synth_en_fr_hi_120 --model_type=roberta --config_name=../config/roberta_8/roberta_base_8_512.json --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 120 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name syntax_english_en_frN_hiV_120   &, https://wandb.ai/ameet-1997/multilingual_syntax/runs/3ijawgzq?workspace=user-ameet-1997

Ameet,3/21/2021,mult-p-2,tpu-2,[grad acc one_to_one_mapping_80] Using larger batch sizes to improve convergence,nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/eng_word_modif_one_to_one_80_grad_acc --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 10000 --gradient_accumulation_steps 4 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name one_to_one_mapping_80_grad_acc --one_to_one_mapping --word_modification add  &, https://wandb.ai/ameet-1997/mutlilingual_word/runs/1lagj3r1?workspace=user-ameet-1997

Ameet,3/25/2021,mult-p-1,tpu-1, [one_to_one_mapping_80 extra warm] Increasing the number of warmup steps to improve convergence,nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/eng_word_modif_one_to_one_80_warmup_30k --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 30000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name one_to_one_mapping_80_warmup_30k --one_to_one_mapping --word_modification add  &, https://wandb.ai/ameet-1997/mutlilingual_word/runs/funakefp?workspace=user-ameet-1997

Ameet,3/25/2021,mult-p-2,tpu-2, [one_to_one_mapping replace- not add] Instead of creating a concatenated dataset- create a replaced dataset,nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/eng_word_modif_one_to_one_replace --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name one_to_one_mapping_replace --one_to_one_mapping --word_modification replace  &,https://wandb.ai/ameet-1997/mutlilingual_word/runs/b9yjkgw8?workspace=user-ameet-1997

Ameet,3/25/2021,ameet-mult-p-3,tpu-3, [one_to_one_mapping smaller model] Train a 8 layer- 8 attention heads- 512 hidden size model instead of 12-12-768, nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/eng_word_modif_one_to_one_8_8_512 --model_type=roberta --config_name=../config/roberta_8/roberta_base_8_512.json --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 40 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name one_to_one_mapping_80_8_8_512 --one_to_one_mapping --word_modification add  & , https://wandb.ai/ameet-1997/mutlilingual_word/runs/3u59he8f?workspace=user-ameet-1997

Ameet,3/27/2021,ameet-mult-p-3,tpu-3, [one_to_one_mapping smaller model longer] Train a 8 layer- 8 attention heads- 512 hidden size model instead of 12-12-768, nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/eng_word_modif_one_to_one_8_8_512 --model_type=roberta --config_name=../config/roberta_8/roberta_base_8_512.json --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name one_to_one_mapping_80_8_8_512 --one_to_one_mapping --word_modification add  & , https://wandb.ai/ameet-1997/mutlilingual_word/runs/2il3qopt?workspace=user-ameet-1997

Ameet,3/27/2021,mult-p-2,tpu-2, [model_vocab_expand] Only expand the vocabulary of the model and train on the same dataset,nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/wikitext_mono_vocab_expand --model_type=roberta --config_name=roberta-base --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 80 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name vocab_expand --one_to_one_mapping &, https://wandb.ai/ameet-1997/mutlilingual_word/runs/2mlstpwl?workspace=user-ameet-1997

Ameet,4/5/2021,mult-p-1,tpu-1, [english_mlm_240] English monolingual model (8/512) trained for 240 epochs (around 420 000 steps),nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_mono_240 --model_type=roberta --config_name=../config/roberta_8/roberta_base_8_512.json --tokenizer_name=roberta-base --learning_rate 1e-4 --num_train_epochs 240 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name english_mono_240 &, https://wandb.ai/ameet-1997/mutlilingual_word/runs/2bby5f32?workspace=user-ameet-1997

Ameet,4/6/2021,mult-p-1,tpu-1, [english_mlm_240_highlr] English monolingual model (8/512) trained for 240 epochs (around 420 000 steps) with 1e-3 lr,nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_mono_240_highlr --model_type=roberta --config_name=../config/roberta_8/roberta_base_8_512.json --tokenizer_name=roberta-base --learning_rate 1e-3 --num_train_epochs 240 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name english_mono_240_highlr &, https://wandb.ai/ameet-1997/mutlilingual_word/runs/2zverktc?workspace=user-ameet-1997

Ameet,4/6/2021,mult-p-1,tpu-1, [english_mlm_240_highlr] English monolingual model (8/512) trained for 240 epochs (around 420 000 steps) with 5e-4 lr,nohup python examples/xla_spawn.py --num_cores 8 examples/language-modeling/run_mlm_synthetic.py --train_file=../../../bucket/wikitext-103-raw/wiki.train.txt --validation_file=../../../bucket/wikitext-103-raw/wiki.valid.txt --cache_dir=../../../bucket/cache --output_dir=../../../bucket/model_outputs/wikitext/english_mono_240_highlr --model_type=roberta --config_name=../config/roberta_8/roberta_base_8_512.json --tokenizer_name=roberta-base --learning_rate 5e-4 --num_train_epochs 240 --warmup_steps 10000 --do_train --do_eval --save_steps 10000 --logging_steps 50 --per_device_train_batch_size 16 --overwrite_output_dir --run_name english_mono_240_highlr &, https://wandb.ai/ameet-1997/mutlilingual_word/runs/2mfjp7vf?workspace=user-ameet-1997